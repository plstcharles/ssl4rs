# @package _global_

# To execute this experiment, run one of the entrypoints while specifying this config, e.g.:
#     python train.py experiment=example_mnist_classif_fast
# This particular config is made to run fast on CPU for testing purposes!

defaults:
  - default.yaml
  - /model/optimization: default.yaml  # use default optim settings (adam + step scheduler)
  - override /data: mnist.yaml  # defines the datamodule for MNIST classification tasks
  - override /callbacks: classification.yaml  # adds a few useful callbacks for classification tasks
  - _self_

experiment_name: mnist_with_micro_mlp  # experiment name used in output paths

model:  # specified which LightningModule-derived implementation provides the training/inference logic
  _recursive_: False
  _target_: ssl4rs.models.mnist.MNISTClassifier
  encoder_config:
    _target_: ssl4rs.models.components.simple_mlp.SimpleMLP
    in_channels: 784  # = the pixel count in 28x28 MNIST images
    hidden_channels: [10]
    out_channels: 10
  head_config:
    _target_: ssl4rs.models.components.simple_mlp.SimpleMLP
    in_channels: 10
    hidden_channels: [10]
    out_channels: 10
  loss_config:  # in this case, we'll instantiate the cross-entropy loss module directly
    _target_: torch.nn.CrossEntropyLoss

data:
  datamodule:
    dataloader_fn_map:
      _default_: # this group provides shared (but overridable) settings for all data loader types
        batch_size: 32

trainer:
  precision: 32
  max_epochs: 3

target_metric: valid/accuracy  # name of the metric we will be targeting for hparam optimization
target_metric_mode: max  # optimization mode of the target metric defined above

utils:
  print_config: True

profiler:  # in case we run the data profiler, these are the modified setting to use
  use_parser: False
  batch_count: 100
  loop_count: 10
