# @package _global_
#
# This root config is associated to the 'train.py' entrypoint provided with the framework.
#
# Note: when launching with this config, you likely need an experiment subconfig! For example:
#     python train.py experiment=example_mnist_classif

defaults:
  - utils: default.yaml # utility configs contain general-use-but-not-so-important settings
  - output: default.yaml # output configs define the root path and name patterns to use for logging
  - data: default.yaml # data configs define the datamodule and dataloaders to instantiate
  - model: default.yaml # model configs define the LightningModule-derived models to instantiate
  - callbacks: default.yaml # callbacks configs define pytorch lightning callbacks to be enabled
  - logger: default.yaml # logger configs define what kind of logging to use (e.g. MLFlow, wandb)
  - trainer: default.yaml # trainer configs define the (lightning-derived) trainer to instantiate
  - experiment: null # experiment configs allow for overrides and version control of all settings
  - hparams_search: null # hyperparameter search configs allow for control of sweep settings
  - optional local: default.yaml # optional overriding config for machine/user specific settings
  - _self_
  - debug: null # debugging configs allow for extra logging and deactivation of optimization stuff

run_type: train-test # defines whether to train, test, or train-and-test in case we might skip one
