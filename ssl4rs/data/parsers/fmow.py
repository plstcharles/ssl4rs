"""Implements a data parser for the Functional Map of the World (fMoW) dataset.

See the following URLs for more info on this dataset:
https://arxiv.org/abs/1711.07846
https://github.com/fMoW/dataset
https://spacenet.ai/iarpa-functional-map-of-the-world-fmow/
"""
import functools
import pathlib
import typing

import deeplake
import numpy as np
import torch
import torch.utils.data
import torchvision.io
import torchvision.transforms.functional

import ssl4rs.data.metadata.fmow
import ssl4rs.data.parsers.utils
import ssl4rs.utils.imgproc
import ssl4rs.utils.logging

logger = ssl4rs.utils.logging.get_logger(__name__)


class DeepLakeParserBase(ssl4rs.data.parsers.utils.DeepLakeParser):
    """Provides data parsing functions for the ENTIRE fMoW dataset (i.e. the 'all' subset).

    This class implements getter functions for each of the expected subsets of the fMoW dataset,
    specifically: `get_train_subset`, `get_val_subset`, `get_test_subset`, and `get_seq_subset`.
    Those subsets will be provided in a DeepLakeParser-compatible interface to simplify and
    optimize data loading.

    The deeplake-derived dataloader is only supported when the `parsing_strategy` is `images`, i.e.
    when a single image is expected for each data sample loaded by this trainer. To use an
    instance-based data loading strategy, use a regular `torch.utils.data.DataLoader`.
    """

    metadata = ssl4rs.data.metadata.fmow

    supported_parsing_strategies = [
        "images",  # load all images as separate samples (will revisit the instances many times)
        "instances",  # load all images for each instance at the same time (visits each instance once)
        "instances-with-random-image",  # load a random image for each instance (visits each instance once)
        "instances-with-first-image",  # load the FIRST image for each instance (visits each instance once)
    ]
    """List of supported data batch preparation strategies that are implemented in this parser."""

    supported_decompression_strategies = [
        "defer",  # defer image decompression to later (will have to be done outside the class)
        "deeplake",  # use the deeplake (built-in) decompression implementation (for any format)
        "opencv",  # use the opencv (imdecode) decompression implementation (for jpegs/pngs, to BGR)
        "libjpeg-turbo",  # use the libjpeg-turbo decompression implementation (for jpegs only!)
        "nvjpeg",  # use the nvjpeg decompression implementation via pytorch (for jpegs only!)
    ]
    """List of supported image decompression strategies that are implemented for the fMoW data."""

    def __init__(
        self,
        dataset_path_or_object: typing.Union[typing.AnyStr, pathlib.Path, deeplake.Dataset],
        parsing_strategy: str = "images",
        decompression_strategy: str = "deeplake",
        keep_metadata_dict: bool = False,
        batch_transforms: "ssl4rs.data.BatchTransformType" = None,
        batch_id_prefix: typing.Optional[typing.AnyStr] = None,
        **extra_deeplake_kwargs,
    ):
        """Parses a fMoW deeplake archive or wraps an already-opened object.

        Note that due to the design of this class (and in contrast to the exporter class), all
        datasets should only ever be opened in read-only mode here.

        Note that the instance data (instance identifiers, image indices, labels, and subsets)
        will be loaded directly into memory. This increases memory usage in this class, but that
        should not really be an issue, even with 120k instances.

        Args:
            dataset_path_or_object: path to the deeplake dataset to be read, or deeplake dataset
                object to be wrapped by this reader. Will be set to READ-ONLY if it is not already.
            parsing_strategy: a string identifying the types of data batches generated by this
                parser. See the `supported_parsing_strategies` for more info.
            decompression_strategy: a string identifying the image decompression strategy to use
                when parsing data. See the `supported_decompression_strategies` for more info.
            keep_metadata_dict: defines whether the full metadata (json) dictionaries should be
                kept in the loaded batches or not. These are fairly big, so there might be a
                cost to keeping these around if you are not using them.
            batch_transforms: configuration dictionary or list of transformation operations that
                will be applied to the "raw" batch data read by this class. These should be
                callable objects that expect to receive a batch dictionary, and that also return
                a batch dictionary.
            batch_id_prefix: string used as a prefix in the batch identifiers generated for the
                data samples read by this parser.
            extra_deeplake_kwargs: extra parameters sent to the deeplake dataset constructor.
                Should not be used if an already-opened dataset is provided.
        """
        self.save_hyperparameters(
            ignore=["dataset_path_or_object", "extra_deeplake_kwargs"],
            logger=False,
        )
        super().__init__(
            dataset_path_or_object=dataset_path_or_object,
            batch_transforms=batch_transforms,
            batch_id_prefix=batch_id_prefix,
            save_hyperparams=False,
            **extra_deeplake_kwargs,
        )
        assert parsing_strategy in self.supported_parsing_strategies, f"invalid parsing strategy: {parsing_strategy}"
        self.parsing_strategy = parsing_strategy
        assert (
            decompression_strategy in self.supported_decompression_strategies
        ), f"invalid decompression strategy: {decompression_strategy}"
        self.decompression_strategy = decompression_strategy
        self.keep_metadata_dict = keep_metadata_dict
        self.image_count = len(self.dataset)
        self.instance_count, self.instance_data, self.instance_idx_to_image_idxs_map = self._load_instance_data(
            self.dataset
        )

    @staticmethod
    def _load_instance_data(
        dataset: deeplake.Dataset,
    ) -> typing.Tuple[int, typing.Dict[str, np.ndarray], typing.Dict[int, typing.List[int]]]:
        """Returns instance data (total count, tensors, and an image-idx-to instance-idx map)."""
        logger.debug(f"Preloading fMoW instance metadata for {len(dataset)} images...")
        instance_data = {
            "instance_idxs": dataset.instance[:].numpy().flatten(),
            "instance_labels": dataset.label[:].numpy().flatten(),
            "instance_subset_idxs": dataset.subset[:].numpy().flatten(),
        }
        instance_idx_to_image_idxs_map = {}
        for image_idx, instance_idx in enumerate(instance_data["instance_idxs"]):
            if instance_idx not in instance_idx_to_image_idxs_map:
                instance_idx_to_image_idxs_map[instance_idx] = []
            instance_idx_to_image_idxs_map[instance_idx].append(image_idx)
        instance_count = len(instance_idx_to_image_idxs_map)
        return instance_count, instance_data, instance_idx_to_image_idxs_map

    def __len__(self) -> int:
        """Returns the total size (in terms of instance or image count) of the dataset."""
        if self.parsing_strategy == "images":
            return self.image_count
        else:
            return self.instance_count

    def _get_raw_batch(
        self,
        index: typing.Hashable,
    ) -> typing.Any:
        """Returns a single data batch loaded from the deeplake dataset at a specified index.

        In contrast with the `__getitem__` function, this internal call will not apply transforms.

        This is a custom reimplementation of the base class version that processes sequences of data
        so that they can be batched properly.
        """
        if self.parsing_strategy == "images":
            image_data = self.dataset[index]
            batch_index = index
        else:
            instance_idx = self.dataset.instance[index].numpy().item()
            image_idxs = self.instance_idx_to_image_idxs_map[instance_idx]
            if self.parsing_strategy == "instances-with-random-image":
                image_idx = np.random.choice(image_idxs)
                image_data = self.dataset[image_idx]
                batch_index = (index, image_idx)
            elif self.parsing_strategy == "instances-with-first-image":
                image_data = self.dataset[image_idxs[0]]
                batch_index = (index, image_idxs[0])
            elif self.parsing_strategy == "instances":
                image_data = self.dataset[image_idxs]
                batch_index = index
            else:
                raise NotImplementedError
        batch = _get_batch_from_sample_data(
            image_data=image_data,
            dataset_info=self.dataset.info,
            parsing_strategy=self.parsing_strategy,
            decompression_strategy=self.decompression_strategy,
            keep_metadata_dict=self.keep_metadata_dict,
        )
        batch[self.batch_index_key] = batch_index
        return batch

    def get_dataloader(
        self,
        num_workers: int = 0,
        batch_size: int = 1,
        drop_last: bool = False,
        collate_fn: typing.Optional[typing.Callable] = None,
        pin_memory: bool = False,
        shuffle: bool = False,
        buffer_size: int = 2048,
        use_local_cache: bool = False,
        **deeplake_pytorch_dataloader_kwargs,
    ) -> torch.utils.data.DataLoader:
        """Returns a deeplake data loader for this data parser object."""
        if self.parsing_strategy != "images":
            raise NotImplementedError(
                "the deeplake dataloader should probably not be used to load instances?\n"
                "\t(potential slow down w/ irregular image shapes, no real use case)"
            )
        transforms = torchvision.transforms.Compose(
            [
                functools.partial(
                    _get_batch_from_sample_data,
                    dataset_info=self.dataset.info,
                    parsing_strategy=self.parsing_strategy,
                    decompression_strategy=self.decompression_strategy,
                    keep_metadata_dict=self.keep_metadata_dict,
                ),
                self.batch_transforms,
            ]
        )
        jpeg_decode_method = "numpy" if self.decompression_strategy == "deeplake" else "tobytes"
        decode_method = {
            tensor: "numpy" if tensor != "image" else jpeg_decode_method for tensor in self.dataset.tensors
        }
        dataloader = deeplake.integrations.pytorch.pytorch.dataset_to_pytorch(
            self.dataset,
            num_workers=num_workers,
            batch_size=batch_size,
            drop_last=drop_last,
            collate_fn=collate_fn,
            pin_memory=pin_memory,
            shuffle=shuffle,
            buffer_size=buffer_size,
            use_local_cache=use_local_cache,
            transform=transforms,
            return_index=True,
            decode_method=decode_method,
            **deeplake_pytorch_dataloader_kwargs,
        )
        return dataloader


class DeepLakeParser(DeepLakeParserBase):
    """Derived version of the `DeepLakeParserBase` allowing subsets to be queried as views.

    See the base class for more information.
    """

    def _get_subset(
        self,
        subset: str,
        hparams_override: typing.Optional[ssl4rs.utils.DictConfig] = None,
    ) -> "DeepLakeParserBase":
        """Returns a subset data parser for a specific intersection of dataset instances."""
        assert subset in self.metadata.subset_types, f"unsupported subset: {subset}"
        logger.info(f"Preparing parser for {self.dataset_name}-{subset}")
        subset_view = self.dataset.load_view(subset)
        assert len(subset_view) > 0
        subset_hparams = self.hparams if hparams_override is None else hparams_override
        return DeepLakeParserBase(
            dataset_path_or_object=subset_view,
            **subset_hparams,
        )

    def get_train_subset(
        self,
        hparams_override: typing.Optional[ssl4rs.utils.DictConfig] = None,
    ) -> "DeepLakeParserBase":
        """Returns a `DeepLakeParser`-compatible parser for the fMoW training set."""
        return self._get_subset("train", hparams_override)

    def get_val_subset(
        self,
        hparams_override: typing.Optional[ssl4rs.utils.DictConfig] = None,
    ) -> "DeepLakeParserBase":
        """Returns a `DeepLakeParser`-compatible parser for the fMoW validation set."""
        return self._get_subset("val", hparams_override)

    def get_test_subset(
        self,
        hparams_override: typing.Optional[ssl4rs.utils.DictConfig] = None,
    ) -> "DeepLakeParserBase":
        """Returns a `DeepLakeParser`-compatible parser for the fMoW testing set."""
        return self._get_subset("test", hparams_override)

    def get_seq_subset(
        self,
        hparams_override: typing.Optional[ssl4rs.utils.DictConfig] = None,
    ) -> "DeepLakeParserBase":
        """Returns a `DeepLakeParser`-compatible parser for the fMoW seq set."""
        return self._get_subset("seq", hparams_override)


def _get_batch_from_sample_data(
    image_data: typing.Union[deeplake.Dataset, typing.Dict[str, np.ndarray]],
    dataset_info: typing.Dict[str, typing.Any],
    parsing_strategy: str,
    decompression_strategy: str,
    keep_metadata_dict: bool,
) -> typing.Dict[str, typing.Any]:
    """Converts image data sample(s) into a batch dictionary.

    When `parsing_strategy="instances"`, the images will be already-batched under lists. With other
    parsing strategies, the data will be returned as-is (unbatched).

    The resulting batch dictionary will contain these metadata fields:
        `bbox`: bounding box (in LTWH format) for the instance in the image.
        `gsd`: the ground sampling distance for the image.
        `cloud_cover`: 0-100 coverage of the image obscured by clouds.
        `sun_azimuth`: degrees (0-360) of clockwise rotation off north to the sun.
        `sun_elevation`: degrees (0-90) of elevation from the horizontal to the sun.
    """
    image_compression = dataset_info["image_compression"]
    batch = dict()
    if parsing_strategy == "instances":
        assert isinstance(
            image_data, deeplake.Dataset
        ), "the preloaded batch data input for this function is not supported for instance parsing"
        metadata = [img_data.metadata.dict() for img_data in image_data]
        if keep_metadata_dict:
            batch["metadata"] = metadata
        batch["instance"] = image_data.instance.numpy().flatten()
        batch["label"] = image_data.label.numpy().flatten()
        batch["subset"] = image_data.subset.numpy().flatten()
        batch["bbox"] = [img_data.bbox.numpy() for img_data in image_data]  # should be in LTWH format
        batch["gsd"] = [m["gsd"] for m in metadata]
        batch["cloud_cover"] = [m["cloud_cover"] for m in metadata]
        batch["sun_azimuth"] = [m["sun_azimuth_dbl"] for m in metadata]
        batch["sun_elevation"] = [m["sun_elevation_dbl"] for m in metadata]
        if image_compression == "jpg":
            images = [
                ssl4rs.utils.imgproc.decode_jpeg(img_data.image, decompression_strategy) for img_data in image_data
            ]
            images = [
                torchvision.transforms.functional.to_tensor(image) if not isinstance(image, torch.Tensor) else image
                for image in images
            ]
            batch["image"] = images
        else:
            raise NotImplementedError  # todo: implement me for png/lz4 data
    else:
        if isinstance(image_data, deeplake.Dataset):
            # this branch runs when we use the dataset directly in a regular dataloader
            metadata = image_data.metadata.dict()
            batch["instance"] = image_data.instance.numpy().item()
            batch["label"] = image_data.label.numpy().item()
            batch["subset"] = image_data.subset.numpy().item()
            batch["bbox"] = image_data.bbox.numpy()  # should be in LTWH format
            image = image_data.image
        else:
            # this branch runs when we use the deeplake dataloader (which preloads the tensor vals)
            assert isinstance(image_data, typing.Dict)
            metadata = image_data["metadata"].item()
            batch["instance"] = image_data["instance"].item()
            batch["label"] = image_data["label"].item()
            batch["subset"] = image_data["subset"].item()
            batch["bbox"] = image_data["bbox"]  # should be in LTWH format
            batch["index"] = image_data["index"].item()
            image = image_data["image"]
        if keep_metadata_dict:
            batch["metadata"] = metadata
        batch["gsd"] = metadata["gsd"]
        batch["cloud_cover"] = metadata["cloud_cover"]
        batch["sun_azimuth"] = metadata["sun_azimuth_dbl"]
        batch["sun_elevation"] = metadata["sun_elevation_dbl"]
        if image_compression == "jpg":
            image = ssl4rs.utils.imgproc.decode_jpeg(image, decompression_strategy)
            if isinstance(image, np.ndarray):
                image = torchvision.transforms.functional.to_tensor(image)
            batch["image"] = image
        else:
            raise NotImplementedError  # todo: implement me for png/lz4 data
    return batch
