{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "\n",
    "import ssl4rs.data.parsers.disa\n",
    "import ssl4rs.utils.config\n",
    "import ssl4rs.utils.logging\n",
    "\n",
    "logger = ssl4rs.utils.logging.setup_logging_for_analysis_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f8b21c8c00082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL IF NECESSARY (i.e. if using another local path)\n",
    "dataset_root = ssl4rs.utils.config.get_data_root_dir() / \"ai4h-disa\" / \"india\"\n",
    "assert dataset_root.is_dir(), f\"bad dataset root dir: {dataset_root}\"\n",
    "deeplake_dataset_path = dataset_root / \".deeplake\"\n",
    "assert deeplake_dataset_path.exists(), f\"bad deeplake dataset path: {deeplake_dataset_path}\"\n",
    "logger.info(f\"Ready to parse deeplake dataset at: {deeplake_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f728e98955368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_float(batch: dict) -> dict:\n",
    "    batch[\"image_data\"] = batch[\"image_data\"].astype(np.float32)\n",
    "    return batch\n",
    "\n",
    "\n",
    "dataset_parser = ssl4rs.data.parsers.disa.DeepLakeParser(\n",
    "    dataset_path_or_object=deeplake_dataset_path,  # already-opened object or path to the .deeplake dir\n",
    "    check_integrity=True,  # will run internal checks to make sure the data is clean/good\n",
    "    batch_transforms=[convert_data_to_float],  # converts uint16 raster data to a pytorch-friendly dtype\n",
    ")\n",
    "dataset_parser.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f20805c1e737fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = np.random.randint(len(dataset_parser))\n",
    "sample_data = dataset_parser[sample_idx]\n",
    "logger.info(f\"Displaying preview for {sample_data['location_id']}...\")\n",
    "# note: preview image is always based on the oldest in the image data stack\n",
    "timestamp = sample_data[\"image_metadata\"][0].item()[\"timestamp\"]\n",
    "logger.info(f\"image timestamp: {timestamp}\")\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sample_data[\"location_preview_image\"])\n",
    "\n",
    "plt.title(\"Image Preview\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(sample_data[\"location_preview_roi\"], cmap=plt.cm.gray)\n",
    "plt.title(\"Region of Interest\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(sample_data[\"field_mask\"], cmap=plt.cm.gray)\n",
    "plt.title(\"Field Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29cef9385740ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stats and other interesting things related to the full dataset\n",
    "metadata_lists = dict(\n",
    "    images_per_location=[],\n",
    "    height_per_location=[],\n",
    "    width_per_location=[],\n",
    "    scatter_per_location=[],\n",
    "    fields_per_location=[],\n",
    "    field_area_per_location=[],\n",
    "    field_percent_coverage_per_location=[],\n",
    "    timestamps=[],\n",
    "    time_deltas=[],\n",
    "    area_per_field=[],\n",
    "    gsd_per_image=[],\n",
    "    view_angle_per_image=[],\n",
    "    valid_percent_coverage_per_image=[],\n",
    "    bandwise_mean_per_image=[],\n",
    "    bandwise_std_per_image=[],\n",
    ")\n",
    "\n",
    "band_count = dataset_parser.dataset_info[\"band_count\"]\n",
    "total_location_count = len(dataset_parser)\n",
    "location_idxs = list(range(total_location_count))\n",
    "\n",
    "for location_idx in tqdm.tqdm(location_idxs, desc=\"parsing location data\"):\n",
    "    location_data = dataset_parser[location_idx]\n",
    "    image_count = int(location_data[\"image_count\"].item())\n",
    "    image_metadata = [location_data[\"image_metadata\"][image_idx].item() for image_idx in range(image_count)]\n",
    "    image_data = location_data[\"image_data\"]\n",
    "    roi_data = location_data[\"image_roi\"]\n",
    "    field_mask = location_data[\"field_mask\"]\n",
    "    field_geoms = location_data[\"field_geoms\"]\n",
    "    field_count = len(field_geoms)\n",
    "    field_polygons = [shapely.geometry.Polygon(field_geoms[pidx]) for pidx in range(field_count)]\n",
    "    field_areas = [p.area for p in field_polygons]\n",
    "    metadata_lists[\"images_per_location\"].append(image_count)\n",
    "    metadata_lists[\"height_per_location\"].append(image_data.shape[-2])\n",
    "    metadata_lists[\"width_per_location\"].append(image_data.shape[-1])\n",
    "    metadata_lists[\"scatter_per_location\"].append(location_data[\"field_scatter\"].item())\n",
    "    metadata_lists[\"fields_per_location\"].append(field_count)\n",
    "    metadata_lists[\"field_area_per_location\"].append(sum(field_areas))\n",
    "    metadata_lists[\"field_percent_coverage_per_location\"].append(\n",
    "        (np.count_nonzero(field_mask) / np.prod(field_mask.shape)) * 100\n",
    "    )\n",
    "    timestamps = [\n",
    "        datetime.datetime.strptime(\n",
    "            image_metadata[image_idx][\"properties\"][\"acquired\"],\n",
    "            \"%Y-%m-%dT%H:%M:%S.%fZ\",\n",
    "        )\n",
    "        for image_idx in range(image_count)\n",
    "    ]\n",
    "    metadata_lists[\"timestamps\"].extend(timestamps)\n",
    "    metadata_lists[\"time_deltas\"].extend(\n",
    "        [timestamps[image_idx] - timestamps[image_idx - 1] for image_idx in range(1, image_count)]\n",
    "    )\n",
    "    metadata_lists[\"area_per_field\"].extend(field_areas)\n",
    "    metadata_lists[\"gsd_per_image\"].extend(\n",
    "        [image_metadata[image_idx][\"properties\"][\"gsd\"] for image_idx in range(image_count)]\n",
    "    )\n",
    "    metadata_lists[\"view_angle_per_image\"].extend(\n",
    "        [image_metadata[image_idx][\"properties\"][\"view_angle\"] for image_idx in range(image_count)]\n",
    "    )\n",
    "    metadata_lists[\"valid_percent_coverage_per_image\"].extend(\n",
    "        [\n",
    "            (np.count_nonzero(roi_data[image_idx]) / np.prod(roi_data.shape[1:])) * 100\n",
    "            for image_idx in range(image_count)\n",
    "        ]\n",
    "    )\n",
    "    metadata_lists[\"bandwise_mean_per_image\"].extend(\n",
    "        [\n",
    "            [image_data[image_idx][band_idx][roi_data[image_idx]].mean() for band_idx in range(band_count)]\n",
    "            for image_idx in range(image_count)\n",
    "        ]\n",
    "    )\n",
    "    metadata_lists[\"bandwise_std_per_image\"].extend(\n",
    "        [\n",
    "            [image_data[image_idx][band_idx][roi_data[image_idx]].std() for band_idx in range(band_count)]\n",
    "            for image_idx in range(image_count)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "total_image_count = sum(metadata_lists[\"images_per_location\"])\n",
    "total_field_count = sum(metadata_lists[\"fields_per_location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e54556a39f2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_to_plot_as_hist = [\n",
    "    \"images_per_location\",\n",
    "    \"height_per_location\",\n",
    "    \"width_per_location\",\n",
    "    \"scatter_per_location\",\n",
    "    \"fields_per_location\",\n",
    "    \"field_area_per_location\",\n",
    "    \"field_percent_coverage_per_location\",\n",
    "    \"area_per_field\",\n",
    "    \"gsd_per_image\",\n",
    "    \"view_angle_per_image\",\n",
    "    \"valid_percent_coverage_per_image\",\n",
    "]\n",
    "\n",
    "for array_name in arrays_to_plot_as_hist:\n",
    "    array_data = metadata_lists[array_name]\n",
    "    fig, ax = plt.subplots(figsize=(12, 4), dpi=300)\n",
    "    counts, bin_edges = np.histogram(array_data, bins=30)\n",
    "    bin_widths = np.diff(bin_edges)\n",
    "    ax.bar(bin_edges[:-1], counts, width=bin_widths, edgecolor=\"black\", align=\"edge\")\n",
    "    ax.set_xlabel(\"Values\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(f\"{array_name} distribution\")\n",
    "    min_val = np.min(array_data)\n",
    "    max_val = np.max(array_data)\n",
    "    mean_val = np.mean(array_data)\n",
    "    median_val = np.median(array_data)\n",
    "    num_decimals = max(0, 4 - int(np.floor(np.log10(max_val - min_val))))\n",
    "    subtitle = (\n",
    "        f\"min: {min_val:.{num_decimals}f}, \"\n",
    "        f\"avg: {mean_val:.{num_decimals}f}, \"\n",
    "        f\"median: {median_val:.{num_decimals}f}, \"\n",
    "        f\"max: {max_val:.{num_decimals}f}\"\n",
    "    )\n",
    "    ax.text(0.5, 0.9, subtitle, transform=ax.transAxes, fontsize=\"small\", ha=\"center\", va=\"bottom\")\n",
    "    ax.grid(True)\n",
    "    ax.xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(1))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39036e1c71481f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = metadata_lists[\"timestamps\"]\n",
    "time_deltas = metadata_lists[\"time_deltas\"]\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 9), dpi=300)\n",
    "axes[0].hist(timestamps, bins=100, alpha=0.75, color=\"blue\")\n",
    "axes[0].xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%m-%d, %H:%M:%S\"))\n",
    "axes[0].xaxis.set_major_locator(matplotlib.dates.MonthLocator())\n",
    "axes[0].set_xticks(axes[0].get_xticks())\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "axes[0].grid(True)\n",
    "axes[0].set_title(\"Acquisition timestamps\")\n",
    "timedeltas_in_days = [td.total_seconds() / (60 * 60 * 24) for td in time_deltas]\n",
    "bins = np.logspace(np.log10(min(timedeltas_in_days)), np.log10(max(timedeltas_in_days)), num=50)\n",
    "axes[1].hist(timedeltas_in_days, bins=bins, alpha=0.75, color=\"blue\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_xlabel(\"Time delta (days, log-scale)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_title(\"Time deltas between acquisitions\")\n",
    "axes[1].grid(True)\n",
    "axes[1].minorticks_on()\n",
    "axes[1].xaxis.set_major_locator(matplotlib.ticker.LogLocator(subs=\"all\"))\n",
    "axes[1].xaxis.set_minor_locator(matplotlib.ticker.LogLocator(subs=\"all\"))\n",
    "min_val = np.min(timedeltas_in_days)\n",
    "max_val = np.max(timedeltas_in_days)\n",
    "mean_val = np.mean(timedeltas_in_days)\n",
    "median_val = np.median(timedeltas_in_days)\n",
    "subtitle = f\"min: {min_val:.2f}, avg: {mean_val:.2f}, median: {median_val:.2f}, max: {max_val:.2f}\"\n",
    "axes[1].text(0.5, 0.9, subtitle, transform=axes[1].transAxes, fontsize=\"small\", ha=\"center\", va=\"bottom\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985e1fcab55c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "if band_count == 3:\n",
    "    colors = [\"red\", \"green\", \"blue\"]\n",
    "    labels = dataset_parser.metadata.three_band_descriptions\n",
    "elif band_count == 4:\n",
    "    colors = [\"blue\", \"green\", \"red\", \"darkred\"]\n",
    "    labels = dataset_parser.metadata.four_band_descriptions\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "bandwise_mean_data = np.asarray(metadata_lists[\"bandwise_mean_per_image\"]).T\n",
    "bandwise_std_data = np.asarray(metadata_lists[\"bandwise_std_per_image\"]).T\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 9), dpi=300)\n",
    "print(\"band_mean_values = [\")\n",
    "for band_idx, mean_data in enumerate(bandwise_mean_data):\n",
    "    axes[0].hist(mean_data, bins=50, histtype=\"step\", color=colors[band_idx], label=labels[band_idx])\n",
    "    print(f\"\\t{np.mean(mean_data)},\")\n",
    "print(\"]\")\n",
    "print(\"band_std_values = [\")\n",
    "for band_idx, std_data in enumerate(bandwise_std_data):\n",
    "    axes[1].hist(std_data, bins=50, histtype=\"step\", color=colors[band_idx], label=labels[band_idx])\n",
    "    print(f\"\\t{np.mean(std_data)},\")\n",
    "print(\"]\")\n",
    "axes[0].set_xlabel(\"Mean value\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Image-wise band mean distribution\")\n",
    "axes[0].grid(True)\n",
    "axes[1].set_xlabel(\"Standard deviation\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_title(\"Image-wise band std distribution\")\n",
    "axes[1].grid(True)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe133600140e8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [  # these are the names of tensors we want to work with, and that will be batched\n",
    "    \"location_id\",\n",
    "    \"field_geoms\",\n",
    "    \"field_mask\",\n",
    "    \"image_data\",\n",
    "    \"image_roi\",\n",
    "    \"image_udm2\",\n",
    "]\n",
    "tensor_names_to_collate_manually = [  # anything that is not a numpy array will be manually handled\n",
    "    n for n in tensor_names if not n.startswith(\"image_\") or n != \"field_mask\"\n",
    "]\n",
    "\n",
    "\n",
    "def custom_collate(batches: list[dict]) -> dict:\n",
    "    output = torch.utils.data.default_collate(\n",
    "        [{k: v for k, v in batch.items() if k not in tensor_names_to_collate_manually} for batch in batches]\n",
    "    )\n",
    "    for k in tensor_names_to_collate_manually:\n",
    "        output[k] = [b[k] for b in batches]\n",
    "    return output\n",
    "\n",
    "\n",
    "dataloader = dataset_parser.get_dataloader(  # we will create a dataloader using deeplake directly\n",
    "    batch_size=1,\n",
    "    collate_fn=custom_collate,\n",
    "    tensors=tensor_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095dc12017eb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8f022acc17e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
